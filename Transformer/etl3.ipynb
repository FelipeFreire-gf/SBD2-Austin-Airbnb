{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b32da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando arquivos em: /home/gandalfe/Documentos/sbd2/SBD2-Austin-Airbnb/Data Layer/raw\n",
      "✅ Arquivo Listings ENCONTRADO!\n",
      "✅ Arquivo Calendar ENCONTRADO!\n",
      "✅ Arquivo Reviews ENCONTRADO!\n",
      "\n",
      " Sucesso! Os DataFrames da camada RAW foram carregados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Ajustando o caminho para navegar entre as pastas\n",
    "# os.path.dirname(os.getcwd()) -> Sobe para a raiz (SBD2-Austin-Airbnb)\n",
    "# Depois entra em 'Data Layer' e depois em 'raw'\n",
    "RAIZ_PROJETO = os.path.dirname(os.getcwd())\n",
    "BASE_PATH = os.path.join(RAIZ_PROJETO, \"Data Layer\", \"raw\")\n",
    "\n",
    "print(f\"Procurando arquivos em: {BASE_PATH}\")\n",
    "\n",
    "# 2. Dicionário com os nomes exatos dos arquivos\n",
    "arquivos = {\n",
    "    \"Listings\": \"dados_brutos_listings.csv\",\n",
    "    \"Calendar\": \"dados_brutos_calendar.csv\",\n",
    "    \"Reviews\": \"dados_brutos_reviews.csv\"\n",
    "}\n",
    "\n",
    "# 3. Validação de existência\n",
    "for nome, arquivo in arquivos.items():\n",
    "    caminho_completo = os.path.join(BASE_PATH, arquivo)\n",
    "    if os.path.exists(caminho_completo):\n",
    "        print(f\"✅ Arquivo {nome} ENCONTRADO!\")\n",
    "    else:\n",
    "        print(f\"❌ ERRO: Arquivo {nome} NÃO encontrado em: {caminho_completo}\")\n",
    "\n",
    "# 4. Iniciar Spark e ler os arquivos\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Austin_Airbnb\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Lendo usando o caminho correto montado acima\n",
    "    df_listings_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Listings\"]), header=True, inferSchema=True)\n",
    "    df_calendar_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Calendar\"]), header=True, inferSchema=True)\n",
    "    df_reviews_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Reviews\"]), header=True, inferSchema=True)\n",
    "    \n",
    "    print(\"\\n Sucesso! Os DataFrames da camada RAW foram carregados.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Falha na leitura: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3f920a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviando 1,033,610 linhas para o banco...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESSO! O banco de dados foi populado.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1. Aplicando try_cast em massa para ignorar erros de deslocamento de coluna no CSV\n",
    "df_listings = df_listings_raw.select(\n",
    "    F.expr(\"try_cast(id as int)\").alias(\"listing_id\"),\n",
    "    F.col(\"name\").alias(\"listing_name\"),\n",
    "    \"property_type\", \n",
    "    \"room_type\", \n",
    "    \"bed_type\", \n",
    "    F.expr(\"try_cast(accommodates as int)\").alias(\"accommodates\"),\n",
    "    F.expr(\"try_cast(bathrooms as double)\").alias(\"bathrooms\"),\n",
    "    F.expr(\"try_cast(bedrooms as double)\").alias(\"bedrooms\"),\n",
    "    F.expr(\"try_cast(beds as double)\").alias(\"beds\"),\n",
    "    \"neighbourhood_cleansed\",\n",
    "    # Limpeza de preço com try_cast\n",
    "    F.expr(\"try_cast(regexp_replace(price, '[^0-9.]', '') as decimal(10,2))\").alias(\"listing_price\"),\n",
    "    F.expr(\"try_cast(number_of_reviews as int)\").alias(\"number_of_reviews\"),\n",
    "    F.expr(\"try_cast(first_review as date)\").alias(\"first_review\"),\n",
    "    F.expr(\"try_cast(last_review as date)\").alias(\"last_review\"),\n",
    "    F.expr(\"try_cast(host_id as int)\").alias(\"host_id\"),\n",
    "    \"host_name\"\n",
    ").fillna({\n",
    "    \"listing_price\": 0.0, \n",
    "    \"number_of_reviews\": 0, \n",
    "    \"accommodates\": 0,\n",
    "    \"bedrooms\": 0,\n",
    "    \"beds\": 0,\n",
    "    \"bathrooms\": 0\n",
    "})\n",
    "\n",
    "# 2. Garantir que listing_id não seja nulo (pois é PK no banco)\n",
    "df_listings = df_listings.filter(F.col(\"listing_id\").isNotNull())\n",
    "\n",
    "# 3. Tratamento do Calendar (também com try_cast por segurança)\n",
    "df_calendar = df_calendar_raw.select(\n",
    "    F.expr(\"try_cast(listing_id as int)\").alias(\"listing_id\"),\n",
    "    F.expr(\"try_cast(date as date)\").alias(\"calendar_date\"),\n",
    "    F.when(F.col(\"available\") == \"t\", True).otherwise(False).alias(\"calendar_available\")\n",
    ").filter(F.col(\"listing_id\").isNotNull() & F.col(\"calendar_date\").isNotNull())\n",
    "\n",
    "# 4. Join Final\n",
    "df_silver = df_calendar.join(df_listings, \"listing_id\", \"inner\")\n",
    "\n",
    "# 5. Configuração JDBC e Gravação no Postgres\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5433/austin_airbnb\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"Enviando {df_silver.count():,} linhas para o banco...\")\n",
    "\n",
    "try:\n",
    "    df_silver.write.jdbc(\n",
    "        url=jdbc_url, \n",
    "        table=\"silver.one_big_table\", \n",
    "        mode=\"append\", # 'append' porque a tabela já existe via DDL\n",
    "        properties=db_properties\n",
    "    )\n",
    "    print(\"SUCESSO! O banco de dados foi populado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro na carga: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3c341e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no banco: 1,033,610\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(host=\"localhost\", port=\"5433\", database=\"austin_airbnb\", user=\"postgres\", password=\"postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT count(*) FROM silver.one_big_table\")\n",
    "print(f\"Total de registros no banco: {cur.fetchone()[0]:,}\")\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a108eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela limpa! Pode rodar a carga do Spark agora.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(host=\"localhost\", port=\"5433\", database=\"austin_airbnb\", user=\"postgres\", password=\"postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"TRUNCATE TABLE silver.one_big_table;\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Tabela limpa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
