{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b32da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Procurando arquivos em: /home/gandalfe/Documentos/sbd2/SBD2-Austin-Airbnb/Data Layer/raw\n",
      "‚úÖ Arquivo Listings ENCONTRADO!\n",
      "‚úÖ Arquivo Calendar ENCONTRADO!\n",
      "‚úÖ Arquivo Reviews ENCONTRADO!\n",
      "\n",
      "üöÄ Sucesso! Os DataFrames da camada RAW foram carregados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Como o notebook est√° em /Transformer e o na raiz\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), '.env'))\n",
    "\n",
    "# 2. CONFIGURAR CAMINHOS BASEADOS NA ESTRUTURA DE PASTAS\n",
    "RAIZ_PROJETO = os.path.dirname(os.getcwd())\n",
    "BASE_PATH = os.path.join(RAIZ_PROJETO, \"Data Layer\", \"raw\")\n",
    "\n",
    "print(f\"üîç Procurando arquivos em: {BASE_PATH}\")\n",
    "\n",
    "# 3. DICION√ÅRIO E VALIDA√á√ÉO\n",
    "arquivos = {\n",
    "    \"Listings\": \"dados_brutos_listings.csv\",\n",
    "    \"Calendar\": \"dados_brutos_calendar.csv\",\n",
    "    \"Reviews\": \"dados_brutos_reviews.csv\"\n",
    "}\n",
    "\n",
    "for nome, arquivo in arquivos.items():\n",
    "    if os.path.exists(os.path.join(BASE_PATH, arquivo)):\n",
    "        print(f\" Arquivo {nome} ENCONTRADO!\")\n",
    "    else:\n",
    "        print(f\" ERRO: Arquivo {nome} N√ÉO encontrado!\")\n",
    "\n",
    "# 4. INICIAR SPARK \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Austin_Airbnb\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 5. EXTRA√á√ÉO\n",
    "try:\n",
    "    df_listings_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Listings\"]), header=True, inferSchema=True)\n",
    "    df_calendar_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Calendar\"]), header=True, inferSchema=True)\n",
    "    df_reviews_raw = spark.read.csv(os.path.join(BASE_PATH, arquivos[\"Reviews\"]), header=True, inferSchema=True)\n",
    "    print(\"\\n Sucesso! Os DataFrames da camada RAW foram carregados.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Falha na leitura: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f920a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviando 1,033,610 linhas para o banco...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/14 17:18:26 ERROR Executor: Exception in task 4.0 in stage 41.0 (TID 101)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (5785387,'2016-02-07 -02'::date,'TRUE','Cute Shabby Chic Room in E. Austin','House','Private room','Real Bed',3,1.0,1.0,2.0,78702,0.00,0,NULL,NULL,6291345,'Beth') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5785387, 2016-02-07) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5785387, 2016-02-07) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 0.0 in stage 41.0 (TID 97)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (72635,'2015-11-07 -02'::date,'FALSE','3 Private Bedrooms, SW Austin','House','Private room','Real Bed',6,2.5,1.0,3.0,78739,0.00,0,NULL,NULL,377044,'Jason & Marcie') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(72635, 2015-11-07) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(72635, 2015-11-07) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 3.0 in stage 41.0 (TID 100)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (7258262,'2016-04-20 -03'::date,'FALSE','Bright, Clean & Modern in Tarrytown','House','Entire home/apt','Real Bed',6,2.5,3.0,2.0,78703,0.00,0,NULL,NULL,9704934,'Shawna') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(7258262, 2016-04-20) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(7258262, 2016-04-20) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 2.0 in stage 41.0 (TID 99)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2429409,'2016-09-27 -03'::date,'TRUE','Artist Home Close to SoCo/Downtown','House','Private room','Real Bed',2,1.0,1.0,1.0,78741,0.00,2,NULL,NULL,2738981,'Billy Joe') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2429409, 2016-09-27) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2429409, 2016-09-27) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 6.0 in stage 41.0 (TID 103)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (3997642,'2016-10-20 -02'::date,'TRUE','Private Treehouse Studio NW Hills','Apartment','Entire home/apt','Real Bed',2,1.0,0.0,1.0,78731,0.00,0,NULL,NULL,19813054,'Marcus') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(3997642, 2016-10-20) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(3997642, 2016-10-20) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 5.0 in stage 41.0 (TID 102)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2420168,'2016-03-16 -03'::date,'FALSE',' EAST AUSTIN 3 BDRM Artist Home','House','Entire home/apt','Real Bed',5,1.5,3.0,3.0,78702,0.00,6,NULL,NULL,791464,'Rachel') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 ERROR Executor: Exception in task 1.0 in stage 41.0 (TID 98)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (5447286,'2016-01-10 -02'::date,'FALSE','Cute SXSW Studio in Hyde Park ','Apartment','Entire home/apt','Real Bed',2,1.0,0.0,1.0,78751,0.00,1,'1125-01-01 -03'::date,NULL,6902508,'Natasha') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5447286, 2016-01-10) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5447286, 2016-01-10) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 5.0 in stage 41.0 (TID 102) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2420168,'2016-03-16 -03'::date,'FALSE',' EAST AUSTIN 3 BDRM Artist Home','House','Entire home/apt','Real Bed',5,1.5,3.0,3.0,78702,0.00,6,NULL,NULL,791464,'Rachel') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 ERROR TaskSetManager: Task 5 in stage 41.0 failed 1 times; aborting job\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 2.0 in stage 41.0 (TID 99) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2429409,'2016-09-27 -03'::date,'TRUE','Artist Home Close to SoCo/Downtown','House','Private room','Real Bed',2,1.0,1.0,1.0,78741,0.00,2,NULL,NULL,2738981,'Billy Joe') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2429409, 2016-09-27) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2429409, 2016-09-27) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 6.0 in stage 41.0 (TID 103) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (3997642,'2016-10-20 -02'::date,'TRUE','Private Treehouse Studio NW Hills','Apartment','Entire home/apt','Real Bed',2,1.0,0.0,1.0,78731,0.00,0,NULL,NULL,19813054,'Marcus') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(3997642, 2016-10-20) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(3997642, 2016-10-20) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 4.0 in stage 41.0 (TID 101) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (5785387,'2016-02-07 -02'::date,'TRUE','Cute Shabby Chic Room in E. Austin','House','Private room','Real Bed',3,1.0,1.0,2.0,78702,0.00,0,NULL,NULL,6291345,'Beth') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5785387, 2016-02-07) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5785387, 2016-02-07) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 0.0 in stage 41.0 (TID 97) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (72635,'2015-11-07 -02'::date,'FALSE','3 Private Bedrooms, SW Austin','House','Private room','Real Bed',6,2.5,1.0,3.0,78739,0.00,0,NULL,NULL,377044,'Jason & Marcie') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(72635, 2015-11-07) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(72635, 2015-11-07) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 3.0 in stage 41.0 (TID 100) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (7258262,'2016-04-20 -03'::date,'FALSE','Bright, Clean & Modern in Tarrytown','House','Entire home/apt','Real Bed',6,2.5,3.0,2.0,78703,0.00,0,NULL,NULL,9704934,'Shawna') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(7258262, 2016-04-20) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(7258262, 2016-04-20) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "26/01/14 17:18:26 WARN TaskSetManager: Lost task 1.0 in stage 41.0 (TID 98) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (5447286,'2016-01-10 -02'::date,'FALSE','Cute SXSW Studio in Hyde Park ','Apartment','Entire home/apt','Real Bed',2,1.0,0.0,1.0,78751,0.00,1,'1125-01-01 -03'::date,NULL,6902508,'Natasha') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5447286, 2016-01-10) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(5447286, 2016-01-10) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na carga: An error occurred while calling o438.jdbc.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 41.0 failed 1 times, most recent failure: Lost task 5.0 in stage 41.0 (TID 102) (192.168.1.8 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2420168,'2016-03-16 -03'::date,'FALSE',' EAST AUSTIN 3 BDRM Artist Home','House','Entire home/apt','Real Bed',5,1.5,3.0,3.0,78702,0.00,6,NULL,NULL,791464,'Rachel') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1496)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2252)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2250)\n",
      "\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:80)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:239)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:592)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n",
      "\t\tat scala.Option.foreach(Option.scala:437)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n",
      "\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n",
      "\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\t\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\t\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1496)\n",
      "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2252)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2250)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1496)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:1017)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:80)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 18 more\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO silver.one_big_table (\"listing_id\",\"calendar_date\",\"calendar_available\",\"listing_name\",\"property_type\",\"room_type\",\"bed_type\",\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"neighbourhood_cleansed\",\"listing_price\",\"number_of_reviews\",\"first_review\",\"last_review\",\"host_id\",\"host_name\") VALUES (2420168,'2016-03-16 -03'::date,'FALSE',' EAST AUSTIN 3 BDRM Artist Home','House','Entire home/apt','Real Bed',5,1.5,3.0,3.0,78702,0.00,6,NULL,NULL,791464,'Rachel') was aborted: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:860)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:1018)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:1017)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2536)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"one_big_table_pkey\"\n",
      "  Detalhe: Key (listing_id, calendar_date)=(2420168, 2016-03-16) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\t... 24 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1. Aplicando try_cast em massa para ignorar erros de deslocamento de coluna no CSV\n",
    "df_listings = df_listings_raw.select(\n",
    "    F.expr(\"try_cast(id as int)\").alias(\"listing_id\"),\n",
    "    F.col(\"name\").alias(\"listing_name\"),\n",
    "    \"property_type\", \n",
    "    \"room_type\", \n",
    "    \"bed_type\", \n",
    "    F.expr(\"try_cast(accommodates as int)\").alias(\"accommodates\"),\n",
    "    F.expr(\"try_cast(bathrooms as double)\").alias(\"bathrooms\"),\n",
    "    F.expr(\"try_cast(bedrooms as double)\").alias(\"bedrooms\"),\n",
    "    F.expr(\"try_cast(beds as double)\").alias(\"beds\"),\n",
    "    \"neighbourhood_cleansed\",\n",
    "    # Limpeza de pre√ßo com try_cast\n",
    "    F.expr(\"try_cast(regexp_replace(price, '[^0-9.]', '') as decimal(10,2))\").alias(\"listing_price\"),\n",
    "    F.expr(\"try_cast(number_of_reviews as int)\").alias(\"number_of_reviews\"),\n",
    "    F.expr(\"try_cast(first_review as date)\").alias(\"first_review\"),\n",
    "    F.expr(\"try_cast(last_review as date)\").alias(\"last_review\"),\n",
    "    F.expr(\"try_cast(host_id as int)\").alias(\"host_id\"),\n",
    "    \"host_name\"\n",
    ").fillna({\n",
    "    \"listing_price\": 0.0, \n",
    "    \"number_of_reviews\": 0, \n",
    "    \"accommodates\": 0,\n",
    "    \"bedrooms\": 0,\n",
    "    \"beds\": 0,\n",
    "    \"bathrooms\": 0\n",
    "})\n",
    "\n",
    "# 2. Garantir que listing_id n√£o seja nulo (pois √© PK no banco)\n",
    "df_listings = df_listings.filter(F.col(\"listing_id\").isNotNull())\n",
    "\n",
    "# 3. Tratamento do Calendar (tamb√©m com try_cast por seguran√ßa)\n",
    "df_calendar = df_calendar_raw.select(\n",
    "    F.expr(\"try_cast(listing_id as int)\").alias(\"listing_id\"),\n",
    "    F.expr(\"try_cast(date as date)\").alias(\"calendar_date\"),\n",
    "    F.when(F.col(\"available\") == \"t\", True).otherwise(False).alias(\"calendar_available\")\n",
    ").filter(F.col(\"listing_id\").isNotNull() & F.col(\"calendar_date\").isNotNull())\n",
    "\n",
    "# 4. Join Final\n",
    "df_silver = df_calendar.join(df_listings, \"listing_id\", \"inner\")\n",
    "\n",
    "# 5. Configura√ß√£o JDBC e Grava√ß√£o no Postgres\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5433/austin_airbnb\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"Enviando {df_silver.count():,} linhas para o banco...\")\n",
    "\n",
    "try:\n",
    "    df_silver.write.jdbc(\n",
    "        url=jdbc_url, \n",
    "        table=\"silver.one_big_table\", \n",
    "        mode=\"append\", # 'append' porque a tabela j√° existe via DDL\n",
    "        properties=db_properties\n",
    "    )\n",
    "    print(\"SUCESSO! O banco de dados foi populado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro na carga: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c341e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no banco: 1,033,610\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(host=\"localhost\", port=\"5433\", database=\"austin_airbnb\", user=\"postgres\", password=\"postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT count(*) FROM silver.one_big_table\")\n",
    "print(f\"Total de registros no banco: {cur.fetchone()[0]:,}\")\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a108eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela limpa\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(host=\"localhost\", port=\"5433\", database=\"austin_airbnb\", user=\"postgres\", password=\"postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"TRUNCATE TABLE silver.one_big_table;\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Tabela limpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04d5566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exibindo 10 linhas da tabela silver.one_big_table ---\n",
      "\n",
      "ID: 3997642 | Data: 2016-10-20 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-21 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-22 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-23 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-24 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-25 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-26 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 3997642 | Data: 2016-10-27 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n",
      "ID: 2429409 | Data: 2016-09-27 | Pre√ßo: $0.00 | Nome: Artist Home Close to SoCo/Down...\n",
      "ID: 3997642 | Data: 2016-10-28 | Pre√ßo: $0.00 | Nome: Private Treehouse Studio NW Hi...\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "try:\n",
    "    # 1. Conectar ao banco\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\", \n",
    "        port=\"5433\", \n",
    "        database=\"austin_airbnb\", \n",
    "        user=\"postgres\", \n",
    "        password=\"postgres\"\n",
    "    )\n",
    "    \n",
    "    # 2. Criar um cursor (usando RealDictCursor para ver os nomes das colunas como um dicion√°rio)\n",
    "    cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "    \n",
    "    # 3. Executar a consulta para pegar 10 linhas\n",
    "    query = \"SELECT listing_id, calendar_date, listing_price, listing_name FROM silver.one_big_table LIMIT 10;\"\n",
    "    cur.execute(query)\n",
    "    \n",
    "    # 4. Recuperar os resultados\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(f\"--- Exibindo {len(rows)} linhas da tabela silver.one_big_table ---\\n\")\n",
    "    \n",
    "    # 5. Iterar e imprimir de forma organizada\n",
    "    for row in rows:\n",
    "        print(f\"ID: {row['listing_id']} | Data: {row['calendar_date']} | Pre√ßo: ${row['listing_price']} | Nome: {row['listing_name'][:30]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao consultar o banco: {e}\")\n",
    "\n",
    "finally:\n",
    "    # 6. Fechar conex√£o sempre!\n",
    "    if cur: cur.close()\n",
    "    if conn: conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
